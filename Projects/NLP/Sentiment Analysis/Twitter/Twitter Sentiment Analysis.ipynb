{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#replaced my keys with those in the lab to show as example\n",
    "api = twitter.Api(consumer_key='VXzRzW62biX8KW7A4XycqIeCL',\n",
    "                      consumer_secret='Dr1ak1sdfL2CdpGCp2IWYg3xbOYFWmJ2H3Tm6ZkgMPo5ejqBrY',\n",
    "                      access_token_key='78477561-2SitfsaoG4zvrq5jk1oMGahSgtBvQ9b7noe1XNNSX',\n",
    "                      access_token_secret='vYNqoq4IrLpRINUQdn06aAWaeoSz7G3PNNSSt23XIlx1F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"created_at\": \"Tue Apr 24 00:21:24 +0000 2018\", \"default_profile\": true, \"default_profile_image\": true, \"friends_count\": 1, \"id\": 988573587643682817, \"lang\": \"en\", \"name\": \"Salil Harsulkar\", \"profile_background_color\": \"F5F8FA\", \"profile_image_url\": \"http://abs.twimg.com/sticky/default_profile_images/default_profile_normal.png\", \"profile_link_color\": \"1DA1F2\", \"profile_sidebar_fill_color\": \"DDEEF6\", \"profile_text_color\": \"333333\", \"screen_name\": \"SalilHarsulkar\"}\n"
     ]
    }
   ],
   "source": [
    "print(api.VerifyCredentials())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "i=0\n",
    "f = open('./streamingData.json', 'w')\n",
    "for tweet in api.GetStreamFilter(locations=['-122.75','36.8','-121.75','37.8']):\n",
    "    #print(tweet)\n",
    "    f.write(json.dumps(tweet))\n",
    "    f.write('\\n')    \n",
    "    i+=1\n",
    "    if i==20:\n",
    "        break\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./streamingData.json','r') as json_data:\n",
    "    d=json_data.readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['created_at', 'id', 'id_str', 'text', 'display_text_range', 'source', 'truncated', 'in_reply_to_status_id', 'in_reply_to_status_id_str', 'in_reply_to_user_id', 'in_reply_to_user_id_str', 'in_reply_to_screen_name', 'user', 'geo', 'coordinates', 'place', 'contributors', 'is_quote_status', 'extended_tweet', 'quote_count', 'reply_count', 'retweet_count', 'favorite_count', 'entities', 'favorited', 'retweeted', 'possibly_sensitive', 'filter_level', 'lang', 'timestamp_ms'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_d=json.loads(d[1])\n",
    "json_d.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text for tweet- 0 : Growing up, I saw personally how awful these types of companies can be on low income families just trying to live paycheck to paycheck.\n",
      "text for tweet- 1 : Just listed! Adorable 1920's home w/gorgeous gardens! Walk to Balboa Park, BART, Muni! https://t.co/vYe6SNj4CG $995‚Ä¶ https://t.co/VgBY1Y1PlZ\n",
      "text for tweet- 2 : I was a good girl at the phlebotomist! https://t.co/0mWu0FGVzl\n",
      "text for tweet- 3 : @vyrus001 You need to unlock your Twitter account so I can retweet your post! üòÇ\n",
      "text for tweet- 4 : @SheaSerrano literally did the same thing... I‚Äôm ready https://t.co/LZMwOltEwT\n",
      "text for tweet- 5 : @M_G_abdelhameed Ÿáÿ™ÿ™ÿ≥ÿØ ŸÖŸÜ ÿßŸÑÿ™ÿ±ÿßÿ®!\n",
      "text for tweet- 6 : @stephneehope Blows I know üòî\n",
      "text for tweet- 7 : went to orange theory yesterday + everything hurts + I‚Äôm dying\n",
      "text for tweet- 8 : My motivation for school is no where to be found üòí\n",
      "text for tweet- 9 : @Friendstagram I am.\n",
      "text for tweet- 10 : @jeffayc You seen Schrader's Dog Eat Dog? He plays the straight man to Defoe's lunatic. Like it or not, it is art!\n",
      "text for tweet- 11 : everything is going to be okay everything is going to be okay everything is going to be okay everything is going to‚Ä¶ https://t.co/3fT5TDmwaI\n",
      "text for tweet- 12 : I'm at Loard's Ice Cream in Oakland, CA https://t.co/KuAOWys6Iw\n",
      "text for tweet- 13 : @terrytoons @Joyce_Lin @VirginAmerica @flySFO Except for the high prices\n",
      "text for tweet- 14 : @alice_disney411 „ÅÜ„Çè„Éº„Éº„Éº„ÄÇ\n",
      "  ÂØùËêΩ„Å°„ÅßË¶ã„Çå„Å™„Åã„Å£„ÅüÔºÅÔºÅ„Ç¢„Éº„Ç´„Ç§„ÉñË°å„Å£„Å¶„Åç„Åæ„ÅôÔºÅÔºÅ\n",
      "text for tweet- 15 : @BCalotte Thank you üíï\n",
      "text for tweet- 16 : @torybruno @BFKutter @ulalaunch @NanoRacks I the case of SLS I read somewhere it‚Äôs in a 250 by 70 kilometer ‚Äúorbit‚Äù‚Ä¶ https://t.co/pNZU5NRUxV\n",
      "text for tweet- 17 : ‚ÄúBobbi Day‚Äù will be the new brand. \n",
      "\n",
      "(4)\n",
      "text for tweet- 18 : @SocialPowerOne1  https://t.co/JfGX66xOE9\n",
      "text for tweet- 19 : I'm at Fruitvale in Oakland, CA https://t.co/toQCKt0nFE\n"
     ]
    }
   ],
   "source": [
    "tweets=[]\n",
    "for i in range(len(d)):\n",
    "    print('text for tweet-',i,':',json.loads(d[i])['text'])\n",
    "    tweets.append(json.loads(d[i])['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment analysis -\n",
    "https://nlpforhackers.io/sentiment-analysis-intro/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import required libraries\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn \n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\hsalil89\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download('wordnet')\n",
    "#nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "replacement_dict={\"http\\S+\":\"\",\"@\\S+\":\"\",\"[\\n]\":\"\",\"'m\":\" am\",\"‚Äôve\":\" have\",\"in‚Äô\":\"ing\",\"‚Äôs\":\" is\",\"w/\":\" with \",\"[^a-zA-Z\\s]+\":\"\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "patterns=replacement_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment Analysis pipeline -\n",
    "In this pipeline we perform the below steps -\n",
    "    1. Clean tweet and remove non-characters using Regex\n",
    "    2. Tokenize the tweets to seperate words\n",
    "    3. Remove stop words\n",
    "    4. Remove duplicate words\n",
    "    5. Extract POS tags for each word and convert tags to wordnet format\n",
    "    6. For each word, lemmatize to remove plurals \n",
    "    7. Identify the sentiment and consolidate it for each tweet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet:  Growing up I saw personally how awful these types of companies can be on low income families just trying to live paycheck to paycheck\n",
      "Sentiment score =  -1.125\n",
      "Tweet:  Just listed Adorable s home  with gorgeous gardens Walk to Balboa Park BART Muni   \n",
      "Sentiment score =  1.25\n",
      "Tweet:  I was a good girl at the phlebotomist \n",
      "Sentiment score =  0.75\n",
      "Tweet:   You need to unlock your Twitter account so I can retweet your post \n",
      "Sentiment score =  0.0\n",
      "Tweet:   literally did the same thing Im ready \n",
      "Sentiment score =  0.0\n",
      "Tweet:     \n",
      "Sentiment score =  0.0\n",
      "Tweet:   Blows I know \n",
      "Sentiment score =  0.0\n",
      "Tweet:  went to orange theory yesterday  everything hurts  Im dying\n",
      "Sentiment score =  0.0\n",
      "Tweet:  My motivation for school is no where to be found \n",
      "Sentiment score =  0.0\n",
      "Tweet:   I am\n",
      "Sentiment score =  0.0\n",
      "Tweet:   You seen Schraders Dog Eat Dog He plays the straight man to Defoes lunatic Like it or not it is art\n",
      "Sentiment score =  -0.875\n",
      "Tweet:  everything is going to be okay everything is going to be okay everything is going to be okay everything is going to \n",
      "Sentiment score =  0.0\n",
      "Tweet:  I am at Loards Ice Cream in Oakland CA \n",
      "Sentiment score =  0.0\n",
      "Tweet:      Except for the high prices\n",
      "Sentiment score =  -0.125\n",
      "Tweet:     \n",
      "Sentiment score =  0.0\n",
      "Tweet:   Thank you \n",
      "Sentiment score =  0.0\n",
      "Tweet:      I the case of SLS I read somewhere it is in a  by  kilometer orbit \n",
      "Sentiment score =  0.0\n",
      "Tweet:  Bobbi Day will be the new brand \n",
      "Sentiment score =  0.375\n",
      "Tweet:    \n",
      "Sentiment score =  0.0\n",
      "Tweet:  I am at Fruitvale in Oakland CA \n",
      "Sentiment score =  0.0\n"
     ]
    }
   ],
   "source": [
    "parsed_tweets=[]\n",
    "ptb_wn_pos_dict= { 'NN' : 'n', 'JJ' : 'a', 'RB' : 'a', 'VB' : 'v' }\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for tweet in tweets:\n",
    "    sentiment = 0.0\n",
    "    tokens_count = 0\n",
    "    for pattern in patterns:\n",
    "        tweet = re.sub(pattern, lambda m: replacement_dict.get(m.group()), tweet)\n",
    "    #print(tweet)\n",
    "    parsed_tweets.append(tweet)\n",
    "    #tokenize\n",
    "    tweet_token= nltk.tokenize.word_tokenize(tweet)\n",
    "    #remove stop words\n",
    "    new_tweet=[str(word).lower() for word in tweet_token if str(word).lower() not in stop]\n",
    "    #print(new_tweet)\n",
    "    tweet_tagged=nltk.pos_tag(np.unique(new_tweet))\n",
    "    for word,tag in tweet_tagged:\n",
    "        if ptb_wn_pos_dict.get(tag,'d') in ['n','a','v']:\n",
    "            pos_wn=ptb_wn_pos_dict[tag]\n",
    "            #lemmatize\n",
    "            lemma=lemmatizer.lemmatize(word, pos_wn) \n",
    "            #sentiment extraction\n",
    "            synsets = wn.synsets(lemma, pos=pos_wn)\n",
    "            if not synsets:\n",
    "                continue\n",
    "            #print(lemma,pos_wn) \n",
    "            # Take the first sense, the most common\n",
    "            synset = synsets[0]\n",
    "            swn_synset = swn.senti_synset(synset.name())\n",
    " \n",
    "            sentiment += swn_synset.pos_score() - swn_synset.neg_score()\n",
    "            tokens_count += 1\n",
    "    print('Tweet: ',tweet)\n",
    "    print('Sentiment score = ',sentiment)\n",
    "    \n",
    "    #POS tageach word\n",
    "    #pos_score.append(np.sum([]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
